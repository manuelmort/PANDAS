{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dd8c9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Dec  4 13:28:48 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.65.06              Driver Version: 580.65.06      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA L40S                    On  |   00000000:49:00.0 Off |                    0 |\n",
      "| N/A   48C    P0            159W /  350W |    5465MiB /  46068MiB |     63%   E. Process |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA L40S                    On  |   00000000:61:00.0 Off |                    0 |\n",
      "| N/A   32C    P8             33W /  350W |       0MiB /  46068MiB |      0%   E. Process |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA L40S                    On  |   00000000:C9:00.0 Off |                    0 |\n",
      "| N/A   32C    P8             32W /  350W |       0MiB /  46068MiB |      0%   E. Process |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA L40S                    On  |   00000000:E1:00.0 Off |                    0 |\n",
      "| N/A   31C    P8             33W /  350W |       0MiB /  46068MiB |      0%   E. Process |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A          708855      C   python                                 5456MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce896f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PANDA GRAPH TRANSFORMER - DATA PREPARATION (3-CLASS)\n",
      "======================================================================\n",
      "\n",
      "[1/5] Loading CSV splits...\n",
      "✓ Loaded train CSV: 8492 samples\n",
      "✓ Loaded val CSV: 1062 samples\n",
      "✓ Loaded test CSV: 1062 samples\n",
      "\n",
      "[2/5] Remapping ISUP grades to 3 classes...\n",
      "\n",
      "Original ISUP grade distribution (train):\n",
      "isup_grade\n",
      "0    2323\n",
      "1    2180\n",
      "2    1074\n",
      "3     976\n",
      "4     997\n",
      "5     942\n",
      "Name: count, dtype: int64\n",
      "\n",
      "✓ Remapping applied:\n",
      "  ISUP 0     -> Class 0 (Background)\n",
      "  ISUP 1     -> Class 1 (Benign)\n",
      "  ISUP 2-5   -> Class 2 (Cancerous)\n",
      "\n",
      "New 3-class distribution (train):\n",
      "label_3class\n",
      "0    2323\n",
      "1    2180\n",
      "2    3989\n",
      "Name: count, dtype: int64\n",
      "\n",
      "New 3-class distribution (val):\n",
      "label_3class\n",
      "0    284\n",
      "1    243\n",
      "2    535\n",
      "Name: count, dtype: int64\n",
      "\n",
      "New 3-class distribution (test):\n",
      "label_3class\n",
      "0    285\n",
      "1    243\n",
      "2    534\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Mapping examples:\n",
      "  ISUP 0 (2323 samples) -> Class 0\n",
      "  ISUP 1 (2180 samples) -> Class 1\n",
      "  ISUP 2 (1074 samples) -> Class 2\n",
      "  ISUP 3 ( 976 samples) -> Class 2\n",
      "  ISUP 4 ( 997 samples) -> Class 2\n",
      "  ISUP 5 ( 942 samples) -> Class 2\n",
      "\n",
      "[3/5] Verifying graphs_all...\n",
      "✓ Available slides in graphs_all: 10615\n",
      "\n",
      "[4/5] Creating train_set.txt, val_set.txt, and test_set.txt...\n",
      "✓ Created train_set.txt with 8492 samples\n",
      "✓ Created val_set.txt with 1061 samples\n",
      "  ⚠ 1 val slides missing from graphs_all\n",
      "✓ Created test_set.txt with 1062 samples\n",
      "\n",
      "Verifying class distribution in output files:\n",
      "\n",
      "Train set class distribution:\n",
      "  Class 0:  2323 samples (27.4%)\n",
      "  Class 1:  2180 samples (25.7%)\n",
      "  Class 2:  3989 samples (47.0%)\n",
      "\n",
      "Val set class distribution:\n",
      "  Class 0:   283 samples (26.7%)\n",
      "  Class 1:   243 samples (22.9%)\n",
      "  Class 2:   535 samples (50.4%)\n",
      "\n",
      "Test set class distribution:\n",
      "  Class 0:   285 samples (26.8%)\n",
      "  Class 1:   243 samples (22.9%)\n",
      "  Class 2:   534 samples (50.3%)\n",
      "\n",
      "======================================================================\n",
      "SAMPLE OUTPUT\n",
      "======================================================================\n",
      "\n",
      "First 5 lines of train_set.txt:\n",
      "  panda/eaf66ac783499827a7f09d69793a07d7\t1\n",
      "  panda/ea035a96c752f59a4934a91068b318f2\t1\n",
      "  panda/4c0d7290c72b5495df6c337b1e5dee88\t2\n",
      "  panda/9edf591d741baf346fc1ff0f42dd4ddc\t2\n",
      "  panda/2309b313ab206df1adf09d970478f4c0\t1\n",
      "\n",
      "First 5 lines of val_set.txt:\n",
      "  panda/b3700db879579fef7adf134941382daf\t1\n",
      "  panda/1d7f8f1abfefa25cfae8cc2ede5d847b\t2\n",
      "  panda/02a2dcd6ad8bc1d9ad7fdc04ffb6dff3\t2\n",
      "  panda/af13aae2257702c758b4a1a8d59d278b\t0\n",
      "  panda/7be44aadbb607a7b01a4ffeb792fd36e\t0\n",
      "\n",
      "First 5 lines of test_set.txt:\n",
      "  panda/80c69b5f428d37636414e89a0adaf228\t0\n",
      "  panda/1fa229ec96b3e350801aae58f0628023\t0\n",
      "  panda/8a4f3f6ba6b4107029e77e1b1b75c069\t1\n",
      "  panda/f381888b35f1fac93c009d7cd38d1608\t2\n",
      "  panda/41b01a396f04c9ad63b9382d91f05cb9\t2\n",
      "\n",
      "[5/5] Creating output directories...\n",
      "✓ Created ./graph_transformer/saved_models/\n",
      "✓ Created ./graph_transformer/runs/\n",
      "\n",
      "======================================================================\n",
      "SETUP COMPLETE - READY TO TRAIN!\n",
      "======================================================================\n",
      "\n",
      "Dataset Split:\n",
      "  Training samples:   8,492 (80%)\n",
      "  Validation samples: 1,061 (10%)\n",
      "  Test samples:       1,062 (10%)\n",
      "  Total:              10,615\n",
      "\n",
      "Classes (n_class): 3\n",
      "  - Class 0: Background (ISUP 0)\n",
      "  - Class 1: Benign (ISUP 1)\n",
      "  - Class 2: Cancerous (ISUP 2-5)\n",
      "\n",
      "======================================================================\n",
      "OUTPUT FILES:\n",
      "======================================================================\n",
      "  ./scripts/train_set.txt\n",
      "  ./scripts/val_set.txt\n",
      "  ./scripts/test_set.txt\n",
      "  ./scripts/label_mapping_info.json\n",
      "\n",
      "✓ Saved label mapping info to ./scripts/label_mapping_info.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PANDA GRAPH TRANSFORMER - DATA PREPARATION (3-CLASS)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Load your CSV splits\n",
    "# ============================================================================\n",
    "print(\"\\n[1/5] Loading CSV splits...\")\n",
    "\n",
    "train_csv_path = \"./data/splits/train_split.csv\"\n",
    "val_csv_path = \"./data/splits/val_split.csv\"\n",
    "test_csv_path = \"./data/splits/test_split.csv\"\n",
    "\n",
    "if not os.path.exists(train_csv_path):\n",
    "    print(f\"✗ ERROR: {train_csv_path} not found!\")\n",
    "    exit(1)\n",
    "if not os.path.exists(val_csv_path):\n",
    "    print(f\"✗ ERROR: {val_csv_path} not found!\")\n",
    "    exit(1)\n",
    "if not os.path.exists(test_csv_path):\n",
    "    print(f\"✗ ERROR: {test_csv_path} not found!\")\n",
    "    exit(1)\n",
    "\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "val_df = pd.read_csv(val_csv_path)\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "\n",
    "print(f\"✓ Loaded train CSV: {len(train_df)} samples\")\n",
    "print(f\"✓ Loaded val CSV: {len(val_df)} samples\")\n",
    "print(f\"✓ Loaded test CSV: {len(test_df)} samples\")\n",
    "\n",
    "id_column = 'image_id'\n",
    "label_column = 'isup_grade'\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Define label remapping for 3-class problem\n",
    "# ============================================================================\n",
    "print(f\"\\n[2/5] Remapping ISUP grades to 3 classes...\")\n",
    "\n",
    "print(\"\\nOriginal ISUP grade distribution (train):\")\n",
    "print(train_df[label_column].value_counts().sort_index())\n",
    "\n",
    "# Define mapping: ISUP 0-5 -> 3 classes\n",
    "# Strategy 1: Background, Benign, Cancerous\n",
    "# ISUP 0 -> Class 0 (Background)\n",
    "# ISUP 1 -> Class 1 (Benign/Low grade)\n",
    "# ISUP 2-5 -> Class 2 (Cancerous/High grade)\n",
    "\n",
    "def remap_labels(isup_grade):\n",
    "    \"\"\"\n",
    "    Remap ISUP grades (0-5) to 3 classes:\n",
    "    - Class 0: Background (ISUP 0)\n",
    "    - Class 1: Benign/Low-grade (ISUP 1)\n",
    "    - Class 2: Cancerous/High-grade (ISUP 2-5)\n",
    "    \"\"\"\n",
    "    if isup_grade == 0:\n",
    "        return 0  # Background\n",
    "    elif isup_grade == 1:\n",
    "        return 1  # Benign\n",
    "    else:  # isup_grade in [2, 3, 4, 5]\n",
    "        return 2  # Cancerous\n",
    "\n",
    "# Apply remapping\n",
    "train_df['label_3class'] = train_df[label_column].apply(remap_labels)\n",
    "val_df['label_3class'] = val_df[label_column].apply(remap_labels)\n",
    "test_df['label_3class'] = test_df[label_column].apply(remap_labels)\n",
    "\n",
    "print(\"\\n✓ Remapping applied:\")\n",
    "print(\"  ISUP 0     -> Class 0 (Background)\")\n",
    "print(\"  ISUP 1     -> Class 1 (Benign)\")\n",
    "print(\"  ISUP 2-5   -> Class 2 (Cancerous)\")\n",
    "\n",
    "print(\"\\nNew 3-class distribution (train):\")\n",
    "print(train_df['label_3class'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nNew 3-class distribution (val):\")\n",
    "print(val_df['label_3class'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nNew 3-class distribution (test):\")\n",
    "print(test_df['label_3class'].value_counts().sort_index())\n",
    "\n",
    "# Show mapping examples\n",
    "print(\"\\nMapping examples:\")\n",
    "for i in range(6):\n",
    "    count_train = len(train_df[train_df[label_column] == i])\n",
    "    mapped = remap_labels(i)\n",
    "    print(f\"  ISUP {i} ({count_train:4d} samples) -> Class {mapped}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Verify graphs_all\n",
    "# ============================================================================\n",
    "print(f\"\\n[3/5] Verifying graphs_all...\")\n",
    "\n",
    "graphs_path = \"./feature_extractor/graphs_phikon/panda\"\n",
    "if not os.path.exists(graphs_path):\n",
    "    print(f\"✗ ERROR: {graphs_path} not found!\")\n",
    "    exit(1)\n",
    "\n",
    "available_slides = set(os.listdir(graphs_path))\n",
    "print(f\"✓ Available slides in graphs_all: {len(available_slides)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Create train_set.txt, val_set.txt, and test_set.txt\n",
    "# ============================================================================\n",
    "print(f\"\\n[4/5] Creating train_set.txt, val_set.txt, and test_set.txt...\")\n",
    "\n",
    "os.makedirs(\"./scripts\", exist_ok=True)\n",
    "\n",
    "# Write train_set.txt with remapped labels\n",
    "train_count = 0\n",
    "missing_train = []\n",
    "\n",
    "with open(\"./scripts/train_set.txt\", 'w') as f:\n",
    "    for _, row in train_df.iterrows():\n",
    "        slide_id = str(row[id_column])\n",
    "        label_3class = int(row['label_3class'])  # Use remapped label\n",
    "        \n",
    "        if slide_id not in available_slides:\n",
    "            missing_train.append(slide_id)\n",
    "            continue\n",
    "        \n",
    "        f.write(f\"panda/{slide_id}\\t{label_3class}\\n\")\n",
    "        train_count += 1\n",
    "\n",
    "print(f\"✓ Created train_set.txt with {train_count} samples\")\n",
    "if missing_train:\n",
    "    print(f\"  ⚠ {len(missing_train)} train slides missing from graphs_all\")\n",
    "\n",
    "# Write val_set.txt with remapped labels\n",
    "val_count = 0\n",
    "missing_val = []\n",
    "\n",
    "with open(\"./scripts/val_set.txt\", 'w') as f:\n",
    "    for _, row in val_df.iterrows():\n",
    "        slide_id = str(row[id_column])\n",
    "        label_3class = int(row['label_3class'])  # Use remapped label\n",
    "        \n",
    "        if slide_id not in available_slides:\n",
    "            missing_val.append(slide_id)\n",
    "            continue\n",
    "        \n",
    "        f.write(f\"panda/{slide_id}\\t{label_3class}\\n\")\n",
    "        val_count += 1\n",
    "\n",
    "print(f\"✓ Created val_set.txt with {val_count} samples\")\n",
    "if missing_val:\n",
    "    print(f\"  ⚠ {len(missing_val)} val slides missing from graphs_all\")\n",
    "\n",
    "# Write test_set.txt with remapped labels\n",
    "test_count = 0\n",
    "missing_test = []\n",
    "\n",
    "with open(\"./scripts/test_set.txt\", 'w') as f:\n",
    "    for _, row in test_df.iterrows():\n",
    "        slide_id = str(row[id_column])\n",
    "        label_3class = int(row['label_3class'])  # Use remapped label\n",
    "        \n",
    "        if slide_id not in available_slides:\n",
    "            missing_test.append(slide_id)\n",
    "            continue\n",
    "        \n",
    "        f.write(f\"panda/{slide_id}\\t{label_3class}\\n\")\n",
    "        test_count += 1\n",
    "\n",
    "print(f\"✓ Created test_set.txt with {test_count} samples\")\n",
    "if missing_test:\n",
    "    print(f\"  ⚠ {len(missing_test)} test slides missing from graphs_all\")\n",
    "\n",
    "# Verify class distribution in output files\n",
    "print(\"\\nVerifying class distribution in output files:\")\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "train_labels = []\n",
    "with open(\"./scripts/train_set.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        label = int(line.strip().split('\\t')[1])\n",
    "        train_labels.append(label)\n",
    "\n",
    "val_labels = []\n",
    "with open(\"./scripts/val_set.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        label = int(line.strip().split('\\t')[1])\n",
    "        val_labels.append(label)\n",
    "\n",
    "test_labels = []\n",
    "with open(\"./scripts/test_set.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        label = int(line.strip().split('\\t')[1])\n",
    "        test_labels.append(label)\n",
    "\n",
    "train_counter = Counter(train_labels)\n",
    "val_counter = Counter(val_labels)\n",
    "test_counter = Counter(test_labels)\n",
    "\n",
    "print(f\"\\nTrain set class distribution:\")\n",
    "for cls in sorted(train_counter.keys()):\n",
    "    pct = 100 * train_counter[cls] / len(train_labels)\n",
    "    print(f\"  Class {cls}: {train_counter[cls]:5d} samples ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nVal set class distribution:\")\n",
    "for cls in sorted(val_counter.keys()):\n",
    "    pct = 100 * val_counter[cls] / len(val_labels)\n",
    "    print(f\"  Class {cls}: {val_counter[cls]:5d} samples ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTest set class distribution:\")\n",
    "for cls in sorted(test_counter.keys()):\n",
    "    pct = 100 * test_counter[cls] / len(test_labels)\n",
    "    print(f\"  Class {cls}: {test_counter[cls]:5d} samples ({pct:.1f}%)\")\n",
    "\n",
    "# Show examples\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAMPLE OUTPUT\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nFirst 5 lines of train_set.txt:\")\n",
    "with open(\"./scripts/train_set.txt\", 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i < 5:\n",
    "            print(f\"  {line.strip()}\")\n",
    "\n",
    "print(\"\\nFirst 5 lines of val_set.txt:\")\n",
    "with open(\"./scripts/val_set.txt\", 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i < 5:\n",
    "            print(f\"  {line.strip()}\")\n",
    "\n",
    "print(\"\\nFirst 5 lines of test_set.txt:\")\n",
    "with open(\"./scripts/test_set.txt\", 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i < 5:\n",
    "            print(f\"  {line.strip()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: Create output directories\n",
    "# ============================================================================\n",
    "print(\"\\n[5/5] Creating output directories...\")\n",
    "\n",
    "os.makedirs(\"./graph_transformer/saved_models\", exist_ok=True)\n",
    "os.makedirs(\"./graph_transformer/runs\", exist_ok=True)\n",
    "\n",
    "print(\"✓ Created ./graph_transformer/saved_models/\")\n",
    "print(\"✓ Created ./graph_transformer/runs/\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SETUP COMPLETE - READY TO TRAIN!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nDataset Split:\")\n",
    "print(f\"  Training samples:   {train_count:,} ({100*train_count/(train_count+val_count+test_count):.0f}%)\")\n",
    "print(f\"  Validation samples: {val_count:,} ({100*val_count/(train_count+val_count+test_count):.0f}%)\")\n",
    "print(f\"  Test samples:       {test_count:,} ({100*test_count/(train_count+val_count+test_count):.0f}%)\")\n",
    "print(f\"  Total:              {train_count+val_count+test_count:,}\")\n",
    "\n",
    "print(f\"\\nClasses (n_class): 3\")\n",
    "print(f\"  - Class 0: Background (ISUP 0)\")\n",
    "print(f\"  - Class 1: Benign (ISUP 1)\")  \n",
    "print(f\"  - Class 2: Cancerous (ISUP 2-5)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OUTPUT FILES:\")\n",
    "print(\"=\"*70)\n",
    "print(\"  ./scripts/train_set.txt\")\n",
    "print(\"  ./scripts/val_set.txt\")\n",
    "print(\"  ./scripts/test_set.txt\")\n",
    "print(\"  ./scripts/label_mapping_info.json\")\n",
    "\n",
    "# Save mapping info for reference\n",
    "mapping_info = {\n",
    "    'original_classes': 6,\n",
    "    'new_classes': 3,\n",
    "    'mapping': {\n",
    "        'ISUP_0': 'Class_0_Background',\n",
    "        'ISUP_1': 'Class_1_Benign',\n",
    "        'ISUP_2-5': 'Class_2_Cancerous'\n",
    "    },\n",
    "    'train_distribution': dict(train_counter),\n",
    "    'val_distribution': dict(val_counter),\n",
    "    'test_distribution': dict(test_counter),\n",
    "    'total_samples': {\n",
    "        'train': train_count,\n",
    "        'val': val_count,\n",
    "        'test': test_count,\n",
    "        'total': train_count + val_count + test_count\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('./scripts/label_mapping_info.json', 'w') as f:\n",
    "    json.dump(mapping_info, f, indent=2)\n",
    "\n",
    "print(\"\\n✓ Saved label mapping info to ./scripts/label_mapping_info.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9660a16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier initialized with class weights: tensor([1.13, 1.21, 0.66], device='cuda:0')\n",
      "======================================================================\n",
      "GTP BASELINE EVALUATION - WEIGHTED MODEL\n",
      "======================================================================\n",
      "Samples: 2123\n",
      "\n",
      "  200/2123\n",
      "  400/2123\n",
      "  600/2123\n",
      "  800/2123\n",
      "  1000/2123\n",
      "  1200/2123\n",
      "  1400/2123\n",
      "  1600/2123\n",
      "  1800/2123\n",
      "  2000/2123\n",
      "\n",
      "✓ Predictions: 2123\n",
      "\n",
      "======================================================================\n",
      "BASELINE RESULTS\n",
      "======================================================================\n",
      "Accuracy: 0.5040 (50.40%)\n",
      "\n",
      "★★★ BASELINE QWK: 0.0018 ★★★\n",
      "\n",
      "Confusion Matrix:\n",
      "         Pred0  Pred1  Pred2\n",
      "Actual 0:    1      0    567\n",
      "Actual 1:    0      0    486\n",
      "Actual 2:    0      0   1069\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          C0      1.000     0.002     0.004       568\n",
      "          C1      0.000     0.000     0.000       486\n",
      "          C2      0.504     1.000     0.670      1069\n",
      "\n",
      "    accuracy                          0.504      2123\n",
      "   macro avg      0.501     0.334     0.225      2123\n",
      "weighted avg      0.521     0.504     0.338      2123\n",
      "\n",
      "======================================================================\n",
      "GRADE TARGETS\n",
      "======================================================================\n",
      "★ BASELINE: 0.0018\n",
      "  B+ (5%):  0.0019\n",
      "  A (10%):  0.0020\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix, classification_report\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.dataset import GraphDataset\n",
    "from helper import collate\n",
    "from models.GraphTransformer import Classifier\n",
    "from collections import OrderedDict\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Load data\n",
    "val_file = './scripts/val_set.txt'\n",
    "with open(val_file, 'r') as f:\n",
    "    val_ids = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "dataset_val = GraphDataset('./feature_extractor/graphs_all', val_ids, site='panda')\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=1, collate_fn=collate, num_workers=0, shuffle=False)\n",
    "\n",
    "# Calculate class weights\n",
    "train_file = './scripts/train_set.txt'\n",
    "with open(train_file, 'r') as f:\n",
    "    train_labels = [int(line.split()[1]) for line in f if line.strip() and len(line.split()) == 2]\n",
    "\n",
    "class_counts = np.bincount(train_labels)\n",
    "class_weights = 1.0 / (class_counts + 1e-6)\n",
    "class_weights = class_weights / class_weights.sum() * 3\n",
    "class_weights = torch.FloatTensor(class_weights).cuda()\n",
    "\n",
    "# Load model WITH class weights\n",
    "model = Classifier(n_class=3, n_features=512, class_weights=class_weights)\n",
    "state_dict = torch.load('./graph_transformer/saved_models/GraphCAM_PANDA_WEIGHTED_v2.pth', map_location=device)\n",
    "new_state_dict = OrderedDict((k.replace('module.', ''), v) for k, v in state_dict.items())\n",
    "model.load_state_dict(new_state_dict)\n",
    "model = model.to(device).eval()\n",
    "\n",
    "# Hook to capture logits\n",
    "logits_captured = None\n",
    "def hook_fn(module, input, output):\n",
    "    global logits_captured\n",
    "    if hasattr(output, 'shape') and len(output.shape) == 2 and output.shape[1] == 3:\n",
    "        logits_captured = output.clone()\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        module.register_forward_hook(hook_fn)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GTP BASELINE EVALUATION - WEIGHTED MODEL\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Samples: {len(dataset_val)}\\n\")\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, sample in enumerate(dataloader_val):\n",
    "        if sample is None:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            img = sample[\"image\"][0] if isinstance(sample[\"image\"], list) else sample[\"image\"]\n",
    "            adj = sample[\"adj_s\"][0] if isinstance(sample[\"adj_s\"], list) else sample[\"adj_s\"]\n",
    "            label = sample[\"label\"][0] if isinstance(sample[\"label\"], list) else sample[\"label\"]\n",
    "            \n",
    "            img = img.unsqueeze(0).float().to(device)\n",
    "            adj = adj.unsqueeze(0).float().to(device)\n",
    "            mask = torch.ones(1, img.size(1)).to(device)\n",
    "            label_tensor = torch.tensor([label], dtype=torch.long).to(device)\n",
    "            \n",
    "            logits_captured = None\n",
    "            model(img, label_tensor, adj, mask)\n",
    "            \n",
    "            if logits_captured is not None:\n",
    "                pred = logits_captured.argmax(1).item()\n",
    "                all_preds.append(pred)\n",
    "                all_labels.append(label)\n",
    "            \n",
    "            if (i + 1) % 200 == 0:\n",
    "                print(f\"  {i+1}/{len(dataloader_val)}\")\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "print(f\"\\n✓ Predictions: {len(all_preds)}\\n\")\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "acc = np.mean(all_preds == all_labels)\n",
    "qwk = cohen_kappa_score(all_labels, all_preds, weights='quadratic')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"BASELINE RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Accuracy: {acc:.4f} ({acc*100:.2f}%)\")\n",
    "print(f\"\\n★★★ BASELINE QWK: {qwk:.4f} ★★★\")\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds, labels=[0,1,2])\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"         Pred0  Pred1  Pred2\")\n",
    "print(f\"Actual 0: {cm[0][0]:4d}   {cm[0][1]:4d}   {cm[0][2]:4d}\")\n",
    "print(f\"Actual 1: {cm[1][0]:4d}   {cm[1][1]:4d}   {cm[1][2]:4d}\")\n",
    "print(f\"Actual 2: {cm[2][0]:4d}   {cm[2][1]:4d}   {cm[2][2]:4d}\")\n",
    "\n",
    "print(f\"\\n{classification_report(all_labels, all_preds, target_names=['C0','C1','C2'], digits=3, zero_division=0)}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GRADE TARGETS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"★ BASELINE: {qwk:.4f}\")\n",
    "print(f\"  B+ (5%):  {qwk*1.05:.4f}\")\n",
    "print(f\"  A (10%):  {qwk*1.10:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c73f8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix, classification_report\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.dataset import GraphDataset\n",
    "from helper import collate\n",
    "from models.GraphTransformer import Classifier\n",
    "from collections import OrderedDict\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Load data\n",
    "val_file = './scripts/val_set.txt'\n",
    "with open(val_file, 'r') as f:\n",
    "    val_ids = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "dataset_val = GraphDataset('./feature_extractor/graphs_all', val_ids, site='panda')\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=1, collate_fn=collate, num_workers=0, shuffle=False)\n",
    "\n",
    "# Calculate class weights\n",
    "train_file = './scripts/train_set.txt'\n",
    "with open(train_file, 'r') as f:\n",
    "    train_labels = [int(line.split()[1]) for line in f if line.strip() and len(line.split()) == 2]\n",
    "\n",
    "class_counts = np.bincount(train_labels)\n",
    "class_weights = 1.0 / (class_counts + 1e-6)\n",
    "class_weights = class_weights / class_weights.sum() * 3\n",
    "class_weights = torch.FloatTensor(class_weights).cuda()\n",
    "\n",
    "# Load model WITH class weights\n",
    "model = Classifier(n_class=3, n_features=512, class_weights=class_weights)\n",
    "state_dict = torch.load('./graph_transformer/saved_models/GraphCAM_PANDA_WEIGHTED_v2.pth', map_location=device)\n",
    "new_state_dict = OrderedDict((k.replace('module.', ''), v) for k, v in state_dict.items())\n",
    "model.load_state_dict(new_state_dict)\n",
    "model = model.to(device).eval()\n",
    "\n",
    "# Hook to capture logits\n",
    "logits_captured = None\n",
    "def hook_fn(module, input, output):\n",
    "    global logits_captured\n",
    "    if hasattr(output, 'shape') and len(output.shape) == 2 and output.shape[1] == 3:\n",
    "        logits_captured = output.clone()\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        module.register_forward_hook(hook_fn)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GTP BASELINE EVALUATION - WEIGHTED MODEL\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Samples: {len(dataset_val)}\\n\")\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, sample in enumerate(dataloader_val):\n",
    "        if sample is None:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            img = sample[\"image\"][0] if isinstance(sample[\"image\"], list) else sample[\"image\"]\n",
    "            adj = sample[\"adj_s\"][0] if isinstance(sample[\"adj_s\"], list) else sample[\"adj_s\"]\n",
    "            label = sample[\"label\"][0] if isinstance(sample[\"label\"], list) else sample[\"label\"]\n",
    "            \n",
    "            img = img.unsqueeze(0).float().to(device)\n",
    "            adj = adj.unsqueeze(0).float().to(device)\n",
    "            mask = torch.ones(1, img.size(1)).to(device)\n",
    "            label_tensor = torch.tensor([label], dtype=torch.long).to(device)\n",
    "            \n",
    "            logits_captured = None\n",
    "            model(img, label_tensor, adj, mask)\n",
    "            \n",
    "            if logits_captured is not None:\n",
    "                pred = logits_captured.argmax(1).item()\n",
    "                all_preds.append(pred)\n",
    "                all_labels.append(label)\n",
    "            \n",
    "            if (i + 1) % 200 == 0:\n",
    "                print(f\"  {i+1}/{len(dataloader_val)}\")\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "print(f\"\\n✓ Predictions: {len(all_preds)}\\n\")\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "acc = np.mean(all_preds == all_labels)\n",
    "qwk = cohen_kappa_score(all_labels, all_preds, weights='quadratic')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"BASELINE RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Accuracy: {acc:.4f} ({acc*100:.2f}%)\")\n",
    "print(f\"\\n★★★ BASELINE QWK: {qwk:.4f} ★★★\")\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds, labels=[0,1,2])\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"         Pred0  Pred1  Pred2\")\n",
    "print(f\"Actual 0: {cm[0][0]:4d}   {cm[0][1]:4d}   {cm[0][2]:4d}\")\n",
    "print(f\"Actual 1: {cm[1][0]:4d}   {cm[1][1]:4d}   {cm[1][2]:4d}\")\n",
    "print(f\"Actual 2: {cm[2][0]:4d}   {cm[2][1]:4d}   {cm[2][2]:4d}\")\n",
    "\n",
    "print(f\"\\n{classification_report(all_labels, all_preds, target_names=['C0','C1','C2'], digits=3, zero_division=0)}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GRADE TARGETS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"★ BASELINE: {qwk:.4f}\")\n",
    "print(f\"  B+ (5%):  {qwk*1.05:.4f}\")\n",
    "print(f\"  A (10%):  {qwk*1.10:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "panda_env",
   "language": "python",
   "name": "panda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
