{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "642d5e56",
   "metadata": {},
   "source": [
    "# SimCLR Feature Extraction for Whole Slide Image Analysis\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### What is SimCLR?\n",
    "\n",
    "**SimCLR (Simple Framework for Contrastive Learning of Visual Representations)** is a self-supervised learning method developed by Google Research that learns meaningful visual features without requiring labeled data. Instead of relying on human annotations, SimCLR learns by comparing different augmented views of the same image.\n",
    "\n",
    "### How SimCLR Works\n",
    "\n",
    "The core idea is elegantly simple:\n",
    "\n",
    "1. **Data Augmentation**: Take an image and create two different \"views\" by applying random augmentations (cropping, color jittering, flipping, blurring, etc.)\n",
    "\n",
    "2. **Feature Extraction**: Pass both views through a neural network (ResNet-18 in our case) to get feature representations\n",
    "\n",
    "3. **Contrastive Learning**: Train the network to recognize that these two views came from the *same* image, while distinguishing them from views of *other* images\n",
    "\n",
    "4. **NT-Xent Loss**: The model uses Normalized Temperature-scaled Cross Entropy loss to pull together representations of the same image while pushing apart representations of different images\n",
    "\n",
    "### Why SimCLR for Pathology?\n",
    "\n",
    "Whole slide images (WSIs) present unique challenges:\n",
    "\n",
    "- **Massive size**: A single WSI can be 100,000 × 100,000 pixels\n",
    "- **Limited labels**: Expert pathologist annotations are expensive and time-consuming\n",
    "- **High variability**: Staining differences, tissue artifacts, and scanner variations\n",
    "\n",
    "SimCLR addresses these challenges by:\n",
    "\n",
    "- Learning robust features from **unlabeled tissue patches**\n",
    "- Capturing **histological patterns** (cellular structures, tissue architecture) without explicit supervision\n",
    "- Producing features that **generalize well** to downstream tasks like cancer grading\n",
    "\n",
    "### Our Pipeline\n",
    "\n",
    "```\n",
    "┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐\n",
    "│   Whole Slide   │     │   256×256 px    │     │    SimCLR       │\n",
    "│     Images      │ ──▶ │   Tile Patches  │ ──▶ │   Training      │\n",
    "│   (PANDA)       │     │   (JPEG tiles)  │     │   (ResNet-18)   │\n",
    "└─────────────────┘     └─────────────────┘     └─────────────────┘\n",
    "                                                        │\n",
    "                                                        ▼\n",
    "                                               ┌─────────────────┐\n",
    "                                               │  512-dim Feature│\n",
    "                                               │    Vectors      │\n",
    "                                               └─────────────────┘\n",
    "```\n",
    "\n",
    "1. **Tiling**: WSIs are divided into 256×256 pixel patches at 20× magnification\n",
    "2. **Self-Supervised Training**: SimCLR learns features from ~170,000 patches across ~8,600 training slides\n",
    "3. **Feature Extraction**: The trained encoder produces a 512-dimensional feature vector for each patch\n",
    "4. **Downstream Use**: These features feed into Graph Transformer networks (GTP) for slide-level cancer grading\n",
    "\n",
    "### Training Configuration\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Backbone | ResNet-18 |\n",
    "| Feature Dimension | 512 |\n",
    "| Batch Size | 76 |\n",
    "| Epochs | 20 |\n",
    "| Temperature (τ) | 0.5 |\n",
    "| Optimizer | Adam |\n",
    "| Learning Rate | 3e-4 (with cosine decay) |\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "By pre-training on unlabeled pathology patches, SimCLR learns to identify meaningful tissue patterns—cell nuclei shapes, glandular structures, stromal patterns—that are directly relevant for cancer diagnosis. This self-supervised approach leverages the vast amount of unlabeled data available in digital pathology archives.\n",
    "\n",
    "---\n",
    "\n",
    "## Training Progress\n",
    "\n",
    "The training loss (NT-Xent) decreases as the model learns to distinguish between different tissue patches:\n",
    "\n",
    "- **Initial Loss**: ~6.0 (random features, poor discrimination)\n",
    "- **Final Loss**: ~4.3 (learned features, good discrimination)\n",
    "\n",
    "The decreasing loss indicates the model is successfully learning to:\n",
    "1. Group similar tissue patterns together in feature space\n",
    "2. Separate dissimilar patterns apart\n",
    "3. Become invariant to augmentations (color shifts, rotations, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530b6415",
   "metadata": {},
   "source": [
    "### Training SimCLR using ResNet backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9fb2042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: CUDA-capable device(s) is/are busy or unavailable\nSearch for `cudaErrorDevicesUnavailable' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m     simclr\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 32\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 28\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[0]\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# The assigned GPU always appears as device 0\u001b[39;00m\n\u001b[1;32m     27\u001b[0m dataset \u001b[38;5;241m=\u001b[39m DataSetWrapper(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 28\u001b[0m simclr \u001b[38;5;241m=\u001b[39m \u001b[43mSimCLR\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m simclr\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m/projectnb/ec500kb/projects/Project_1_Team_1/Official_GTP_PANDAS/PANDAS/feature_extractor/simclr.py:38\u001b[0m, in \u001b[0;36mSimCLR.__init__\u001b[0;34m(self, dataset, config)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m SummaryWriter()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m dataset\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnt_xent_criterion \u001b[38;5;241m=\u001b[39m \u001b[43mNTXentLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projectnb/ec500kb/projects/Project_1_Team_1/Official_GTP_PANDAS/PANDAS/feature_extractor/loss/nt_xent.py:13\u001b[0m, in \u001b[0;36mNTXentLoss.__init__\u001b[0;34m(self, device, batch_size, temperature, use_cosine_similarity)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_samples_from_same_repr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_correlated_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mbool)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarity_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_similarity_function(use_cosine_similarity)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/projectnb/ec500kb/projects/Project_1_Team_1/Official_GTP_PANDAS/PANDAS/feature_extractor/loss/nt_xent.py:30\u001b[0m, in \u001b[0;36mNTXentLoss._get_correlated_mask\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     28\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy((diag \u001b[38;5;241m+\u001b[39m l1 \u001b[38;5;241m+\u001b[39m l2))\n\u001b[1;32m     29\u001b[0m mask \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m mask)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mbool)\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nSearch for `cudaErrorDevicesUnavailable' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from simclr import SimCLR\n",
    "import yaml\n",
    "from data_aug.dataset_wrapper import DataSetWrapper\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "# REMOVE THIS LINE - let the scheduler handle GPU assignment\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1,2'\n",
    "\n",
    "def main():\n",
    "    # Filter out ALL Jupyter kernel arguments\n",
    "    sys.argv = [sys.argv[0]]  # Keep only the script name\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--magnification', type=str, default='20x')\n",
    "    args = parser.parse_args()\n",
    "    config = yaml.load(open(\"config.yaml\", \"r\"), Loader=yaml.FullLoader)\n",
    "    \n",
    "    # Use the GPU(s) assigned by the scheduler\n",
    "    # If you requested 1 GPU, use n_gpu=1\n",
    "    config['n_gpu'] = 1\n",
    "    config['gpu_ids'] = \"[0]\"  # The assigned GPU always appears as device 0\n",
    "   \n",
    "    dataset = DataSetWrapper(config['batch_size'], **config['dataset'])\n",
    "    simclr = SimCLR(dataset, config)\n",
    "    simclr.train()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a9d20a",
   "metadata": {},
   "source": [
    "Test Model with weights using 10 random patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "489ef519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SIMCLR MODEL TEST\n",
      "======================================================================\n",
      "Loading trained SimCLR model...\n",
      "Feature extractor: resnet18\n",
      "✓ Model loaded from runs/Nov29_15-00-34_scc-214/checkpoints/model.pth\n",
      "\n",
      "Finding test patches in /projectnb/ec500kb/projects/Project_1_Team_1/PANDA_DATA_MANNY/tiles_10...\n",
      "✓ Found 121860 total patches\n",
      "✓ Testing on 10 random patches\n",
      "\n",
      "Extracting features...\n",
      "----------------------------------------------------------------------\n",
      "[1/10] 11_56.jpeg\n",
      "  Features shape:    (512,)\n",
      "  Projections shape: (512,)\n",
      "  Feature norm:      25.359\n",
      "  Feature mean:      0.577 ± 0.961\n",
      "\n",
      "[2/10] 13_16.jpeg\n",
      "  Features shape:    (512,)\n",
      "  Projections shape: (512,)\n",
      "  Feature norm:      24.897\n",
      "  Feature mean:      0.613 ± 0.914\n",
      "\n",
      "[3/10] 45_4.jpeg\n",
      "  Features shape:    (512,)\n",
      "  Projections shape: (512,)\n",
      "  Feature norm:      27.561\n",
      "  Feature mean:      0.819 ± 0.901\n",
      "\n",
      "[4/10] 7_43.jpeg\n",
      "  Features shape:    (512,)\n",
      "  Projections shape: (512,)\n",
      "  Feature norm:      22.274\n",
      "  Feature mean:      0.531 ± 0.829\n",
      "\n",
      "[5/10] 42_23.jpeg\n",
      "  Features shape:    (512,)\n",
      "  Projections shape: (512,)\n",
      "  Feature norm:      25.526\n",
      "  Feature mean:      0.657 ± 0.917\n",
      "\n",
      "[6/10] 20_42.jpeg\n",
      "  Features shape:    (512,)\n",
      "  Projections shape: (512,)\n",
      "  Feature norm:      25.804\n",
      "  Feature mean:      0.782 ± 0.830\n",
      "\n",
      "[7/10] 11_36.jpeg\n",
      "  Features shape:    (512,)\n",
      "  Projections shape: (512,)\n",
      "  Feature norm:      24.124\n",
      "  Feature mean:      0.619 ± 0.868\n",
      "\n",
      "[8/10] 8_19.jpeg\n",
      "  Features shape:    (512,)\n",
      "  Projections shape: (512,)\n",
      "  Feature norm:      27.453\n",
      "  Feature mean:      0.811 ± 0.903\n",
      "\n",
      "[9/10] 39_46.jpeg\n",
      "  Features shape:    (512,)\n",
      "  Projections shape: (512,)\n",
      "  Feature norm:      30.021\n",
      "  Feature mean:      0.749 ± 1.095\n",
      "\n",
      "[10/10] 5_14.jpeg\n",
      "  Features shape:    (512,)\n",
      "  Projections shape: (512,)\n",
      "  Feature norm:      29.275\n",
      "  Feature mean:      0.891 ± 0.938\n",
      "\n",
      "======================================================================\n",
      "FEATURE ANALYSIS\n",
      "======================================================================\n",
      "Features shape:    (10, 512)\n",
      "Projections shape: (10, 512)\n",
      "\n",
      "Feature statistics:\n",
      "  Mean:     0.7049\n",
      "  Std:      0.9256\n",
      "  Min:      0.0000\n",
      "  Max:      6.3338\n",
      "\n",
      "Feature diversity check:\n",
      "  Pairwise cosine similarity:\n",
      "    Mean: 0.712\n",
      "    Std:  0.147\n",
      "    Min:  0.375\n",
      "    Max:  0.976\n",
      "\n",
      "======================================================================\n",
      "INTERPRETATION\n",
      "======================================================================\n",
      "✓ OK: Features have moderate diversity\n",
      "  → Model provides useful representations\n",
      "\n",
      "Model is ready to use for feature extraction! ✓\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Test SimCLR trained model - extract features from sample patches\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from models.resnet_simclr import ResNetSimCLR\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "CHECKPOINT_PATH = \"runs/Nov29_15-00-34_scc-214/checkpoints/model.pth\"\n",
    "TILES_DIR = \"/projectnb/ec500kb/projects/Project_1_Team_1/PANDA_DATA_MANNY/tiles_10\"\n",
    "NUM_TEST_PATCHES = 10  # Test on 10 random patches\n",
    "\n",
    "# ============================================================\n",
    "# LOAD MODEL\n",
    "# ============================================================\n",
    "def load_trained_model(checkpoint_path):\n",
    "    \"\"\"Load trained SimCLR model.\"\"\"\n",
    "    print(\"Loading trained SimCLR model...\")\n",
    "    \n",
    "    # Initialize model (same as training)\n",
    "    model = ResNetSimCLR(base_model=\"resnet18\", out_dim=512)\n",
    "    \n",
    "    # Load trained weights\n",
    "    state_dict = torch.load(checkpoint_path, map_location='cuda')\n",
    "    \n",
    "    # FIX: Remove 'module.' prefix from DataParallel\n",
    "    from collections import OrderedDict\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k[7:] if k.startswith('module.') else k  # remove 'module.' prefix\n",
    "        new_state_dict[name] = v\n",
    "    \n",
    "    model.load_state_dict(new_state_dict)\n",
    "    \n",
    "    # Set to evaluation mode\n",
    "    model.eval()\n",
    "    model = model.cuda()\n",
    "    \n",
    "    print(f\"✓ Model loaded from {checkpoint_path}\")\n",
    "    return model\n",
    "\n",
    "# ============================================================\n",
    "# FEATURE EXTRACTION\n",
    "# ============================================================\n",
    "def extract_features(model, image_path):\n",
    "    \"\"\"Extract 512-dim features from a patch.\"\"\"\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_tensor = transform(img).unsqueeze(0).cuda()  # Add batch dimension\n",
    "    \n",
    "    # Extract features (no gradients needed)\n",
    "    with torch.no_grad():\n",
    "        h, z = model(img_tensor)  # h = features, z = projections\n",
    "    \n",
    "    return h.cpu().numpy(), z.cpu().numpy()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# TEST\n",
    "# ============================================================\n",
    "def test_model():\n",
    "    \"\"\"Test the trained model on sample patches.\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"SIMCLR MODEL TEST\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load model\n",
    "    model = load_trained_model(CHECKPOINT_PATH)\n",
    "    \n",
    "    # Get sample patches\n",
    "    print(f\"\\nFinding test patches in {TILES_DIR}...\")\n",
    "    all_patches = []\n",
    "    for wsi_dir in os.listdir(TILES_DIR):\n",
    "        wsi_path = os.path.join(TILES_DIR, wsi_dir)\n",
    "        if os.path.isdir(wsi_path):\n",
    "            patches = glob(os.path.join(wsi_path, \"*.jpeg\"))\n",
    "            all_patches.extend(patches)\n",
    "    \n",
    "    # Sample random patches\n",
    "    np.random.shuffle(all_patches)\n",
    "    test_patches = all_patches[:NUM_TEST_PATCHES]\n",
    "    \n",
    "    print(f\"✓ Found {len(all_patches)} total patches\")\n",
    "    print(f\"✓ Testing on {len(test_patches)} random patches\")\n",
    "    print()\n",
    "    \n",
    "    # Extract features\n",
    "    print(\"Extracting features...\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    all_features = []\n",
    "    all_projections = []\n",
    "    \n",
    "    for i, patch_path in enumerate(test_patches):\n",
    "        patch_name = os.path.basename(patch_path)\n",
    "        \n",
    "        # Extract\n",
    "        features, projections = extract_features(model, patch_path)\n",
    "        all_features.append(features)\n",
    "        all_projections.append(projections)\n",
    "        \n",
    "        # Print stats\n",
    "        print(f\"[{i+1}/{len(test_patches)}] {patch_name}\")\n",
    "        print(f\"  Features shape:    {features.shape}\")\n",
    "        print(f\"  Projections shape: {projections.shape}\")\n",
    "        print(f\"  Feature norm:      {np.linalg.norm(features):.3f}\")\n",
    "        print(f\"  Feature mean:      {features.mean():.3f} ± {features.std():.3f}\")\n",
    "        print()\n",
    "    \n",
    "    # Convert to arrays\n",
    "    all_features = np.vstack(all_features)\n",
    "    all_projections = np.vstack(all_projections)\n",
    "    \n",
    "    # Analysis\n",
    "    print(\"=\"*70)\n",
    "    print(\"FEATURE ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Features shape:    {all_features.shape}\")\n",
    "    print(f\"Projections shape: {all_projections.shape}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Feature statistics:\")\n",
    "    print(f\"  Mean:     {all_features.mean():.4f}\")\n",
    "    print(f\"  Std:      {all_features.std():.4f}\")\n",
    "    print(f\"  Min:      {all_features.min():.4f}\")\n",
    "    print(f\"  Max:      {all_features.max():.4f}\")\n",
    "    print()\n",
    "    \n",
    "    # Test similarity (patches should have diverse features)\n",
    "    print(\"Feature diversity check:\")\n",
    "    from scipy.spatial.distance import cosine\n",
    "    \n",
    "    similarities = []\n",
    "    for i in range(len(all_features)):\n",
    "        for j in range(i+1, len(all_features)):\n",
    "            sim = 1 - cosine(all_features[i], all_features[j])\n",
    "            similarities.append(sim)\n",
    "    \n",
    "    similarities = np.array(similarities)\n",
    "    print(f\"  Pairwise cosine similarity:\")\n",
    "    print(f\"    Mean: {similarities.mean():.3f}\")\n",
    "    print(f\"    Std:  {similarities.std():.3f}\")\n",
    "    print(f\"    Min:  {similarities.min():.3f}\")\n",
    "    print(f\"    Max:  {similarities.max():.3f}\")\n",
    "    print()\n",
    "    \n",
    "    # Interpretation\n",
    "    print(\"=\"*70)\n",
    "    print(\"INTERPRETATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if similarities.mean() < 0.5:\n",
    "        print(\"✓ GOOD: Features are diverse (low similarity)\")\n",
    "        print(\"  → Model learned to distinguish different patches\")\n",
    "    elif similarities.mean() < 0.8:\n",
    "        print(\"✓ OK: Features have moderate diversity\")\n",
    "        print(\"  → Model provides useful representations\")\n",
    "    else:\n",
    "        print(\"⚠ WARNING: Features are very similar\")\n",
    "        print(\"  → Model may not have learned well\")\n",
    "    \n",
    "    print()\n",
    "    print(\"Model is ready to use for feature extraction! ✓\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    test_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b74ab20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "IMAGENET PRETRAINED RESNET TEST\n",
      "======================================================================\n",
      "Loading ImageNet pretrained ResNet model...\n",
      "Feature extractor: resnet50\n",
      "✓ ImageNet pretrained model loaded\n",
      "✓ Feature dimension: 2048\n",
      "\n",
      "Finding test patches in /projectnb/ec500kb/projects/Project_1_Team_1/PANDA_DATA_MANNY/tiles_10...\n",
      "✓ Found 121860 total patches\n",
      "✓ Testing on 10 random patches\n",
      "\n",
      "Extracting features...\n",
      "----------------------------------------------------------------------\n",
      "[1/10] 20_28.jpeg\n",
      "  Features shape:    (2048,)\n",
      "  Feature norm:      20.777\n",
      "  Feature mean:      0.292 ± 0.355\n",
      "\n",
      "[2/10] 1_27.jpeg\n",
      "  Features shape:    (2048,)\n",
      "  Feature norm:      24.013\n",
      "  Feature mean:      0.353 ± 0.396\n",
      "\n",
      "[3/10] 1_9.jpeg\n",
      "  Features shape:    (2048,)\n",
      "  Feature norm:      22.675\n",
      "  Feature mean:      0.328 ± 0.379\n",
      "\n",
      "[4/10] 11_29.jpeg\n",
      "  Features shape:    (2048,)\n",
      "  Feature norm:      29.991\n",
      "  Feature mean:      0.446 ± 0.490\n",
      "\n",
      "[5/10] 37_32.jpeg\n",
      "  Features shape:    (2048,)\n",
      "  Feature norm:      28.593\n",
      "  Feature mean:      0.448 ± 0.446\n",
      "\n",
      "[6/10] 18_11.jpeg\n",
      "  Features shape:    (2048,)\n",
      "  Feature norm:      28.697\n",
      "  Feature mean:      0.444 ± 0.453\n",
      "\n",
      "[7/10] 11_41.jpeg\n",
      "  Features shape:    (2048,)\n",
      "  Feature norm:      27.525\n",
      "  Feature mean:      0.384 ± 0.472\n",
      "\n",
      "[8/10] 40_15.jpeg\n",
      "  Features shape:    (2048,)\n",
      "  Feature norm:      25.815\n",
      "  Feature mean:      0.393 ± 0.413\n",
      "\n",
      "[9/10] 11_3.jpeg\n",
      "  Features shape:    (2048,)\n",
      "  Feature norm:      24.101\n",
      "  Feature mean:      0.374 ± 0.379\n",
      "\n",
      "[10/10] 18_56.jpeg\n",
      "  Features shape:    (2048,)\n",
      "  Feature norm:      26.221\n",
      "  Feature mean:      0.355 ± 0.458\n",
      "\n",
      "======================================================================\n",
      "FEATURE ANALYSIS\n",
      "======================================================================\n",
      "Features shape:    (10, 2048)\n",
      "\n",
      "Feature statistics:\n",
      "  Mean:     0.3817\n",
      "  Std:      0.4292\n",
      "  Min:      0.0000\n",
      "  Max:      5.4863\n",
      "\n",
      "Feature diversity check:\n",
      "  Pairwise cosine similarity:\n",
      "    Mean: 0.739\n",
      "    Std:  0.060\n",
      "    Min:  0.617\n",
      "    Max:  0.856\n",
      "\n",
      "======================================================================\n",
      "INTERPRETATION\n",
      "======================================================================\n",
      "✓ OK: Features have moderate diversity\n",
      "  → Model provides useful representations\n",
      "\n",
      "Model is ready to use for feature extraction! ✓\n",
      "======================================================================\n",
      "\n",
      "COMPARISON WITH YOUR SIMCLR:\n",
      "  SimCLR features:  512-dim\n",
      "  ImageNet features: 2048-dim\n",
      "  ImageNet uses standard normalization and is pretrained on 1.2M images\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Test ImageNet pretrained ResNet - extract features from sample patches\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "TILES_DIR = \"/projectnb/ec500kb/projects/Project_1_Team_1/PANDA_DATA_MANNY/tiles_10\"\n",
    "NUM_TEST_PATCHES = 10  # Test on 10 random patches\n",
    "USE_RESNET50 = True  # True for ResNet50 (2048-dim), False for ResNet18 (512-dim)\n",
    "\n",
    "# ============================================================\n",
    "# LOAD MODEL\n",
    "# ============================================================\n",
    "def load_imagenet_model(use_resnet50=True):\n",
    "    \"\"\"Load ImageNet pretrained ResNet model.\"\"\"\n",
    "    print(\"Loading ImageNet pretrained ResNet model...\")\n",
    "    \n",
    "    if use_resnet50:\n",
    "        print(\"Feature extractor: resnet50\")\n",
    "        # ResNet50 - gives 2048-dim features\n",
    "        model = models.resnet50(weights='IMAGENET1K_V1')\n",
    "        feature_dim = 2048\n",
    "    else:\n",
    "        print(\"Feature extractor: resnet18\")\n",
    "        # ResNet18 - gives 512-dim features (same as your SimCLR)\n",
    "        model = models.resnet18(weights='IMAGENET1K_V1')\n",
    "        feature_dim = 512\n",
    "    \n",
    "    # Remove the final classification layer (FC layer)\n",
    "    # Keep everything up to avgpool\n",
    "    model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "    \n",
    "    # Set to evaluation mode\n",
    "    model.eval()\n",
    "    model = model.cuda()\n",
    "    \n",
    "    print(f\"✓ ImageNet pretrained model loaded\")\n",
    "    print(f\"✓ Feature dimension: {feature_dim}\")\n",
    "    return model, feature_dim\n",
    "\n",
    "# ============================================================\n",
    "# FEATURE EXTRACTION\n",
    "# ============================================================\n",
    "def extract_features(model, image_path):\n",
    "    \"\"\"Extract features from a patch using ImageNet pretrained model.\"\"\"\n",
    "    \n",
    "    # ImageNet normalization (IMPORTANT!)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet stats\n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_tensor = transform(img).unsqueeze(0).cuda()  # Add batch dimension\n",
    "    \n",
    "    # Extract features (no gradients needed)\n",
    "    with torch.no_grad():\n",
    "        features = model(img_tensor)  # Shape: [1, feature_dim, 1, 1]\n",
    "        features = features.squeeze()  # Shape: [feature_dim]\n",
    "    \n",
    "    return features.cpu().numpy()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# TEST\n",
    "# ============================================================\n",
    "def test_model():\n",
    "    \"\"\"Test the ImageNet pretrained model on sample patches.\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"IMAGENET PRETRAINED RESNET TEST\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load model\n",
    "    model, feature_dim = load_imagenet_model(use_resnet50=USE_RESNET50)\n",
    "    \n",
    "    # Get sample patches\n",
    "    print(f\"\\nFinding test patches in {TILES_DIR}...\")\n",
    "    all_patches = []\n",
    "    for wsi_dir in os.listdir(TILES_DIR):\n",
    "        wsi_path = os.path.join(TILES_DIR, wsi_dir)\n",
    "        if os.path.isdir(wsi_path):\n",
    "            patches = glob(os.path.join(wsi_path, \"*.jpeg\"))\n",
    "            all_patches.extend(patches)\n",
    "    \n",
    "    # Sample random patches\n",
    "    np.random.shuffle(all_patches)\n",
    "    test_patches = all_patches[:NUM_TEST_PATCHES]\n",
    "    \n",
    "    print(f\"✓ Found {len(all_patches)} total patches\")\n",
    "    print(f\"✓ Testing on {len(test_patches)} random patches\")\n",
    "    print()\n",
    "    \n",
    "    # Extract features\n",
    "    print(\"Extracting features...\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    all_features = []\n",
    "    \n",
    "    for i, patch_path in enumerate(test_patches):\n",
    "        patch_name = os.path.basename(patch_path)\n",
    "        \n",
    "        # Extract\n",
    "        features = extract_features(model, patch_path)\n",
    "        all_features.append(features)\n",
    "        \n",
    "        # Print stats\n",
    "        print(f\"[{i+1}/{len(test_patches)}] {patch_name}\")\n",
    "        print(f\"  Features shape:    {features.shape}\")\n",
    "        print(f\"  Feature norm:      {np.linalg.norm(features):.3f}\")\n",
    "        print(f\"  Feature mean:      {features.mean():.3f} ± {features.std():.3f}\")\n",
    "        print()\n",
    "    \n",
    "    # Convert to array\n",
    "    all_features = np.vstack(all_features)\n",
    "    \n",
    "    # Analysis\n",
    "    print(\"=\"*70)\n",
    "    print(\"FEATURE ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Features shape:    {all_features.shape}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Feature statistics:\")\n",
    "    print(f\"  Mean:     {all_features.mean():.4f}\")\n",
    "    print(f\"  Std:      {all_features.std():.4f}\")\n",
    "    print(f\"  Min:      {all_features.min():.4f}\")\n",
    "    print(f\"  Max:      {all_features.max():.4f}\")\n",
    "    print()\n",
    "    \n",
    "    # Test similarity (patches should have diverse features)\n",
    "    print(\"Feature diversity check:\")\n",
    "    from scipy.spatial.distance import cosine\n",
    "    \n",
    "    similarities = []\n",
    "    for i in range(len(all_features)):\n",
    "        for j in range(i+1, len(all_features)):\n",
    "            sim = 1 - cosine(all_features[i], all_features[j])\n",
    "            similarities.append(sim)\n",
    "    \n",
    "    similarities = np.array(similarities)\n",
    "    print(f\"  Pairwise cosine similarity:\")\n",
    "    print(f\"    Mean: {similarities.mean():.3f}\")\n",
    "    print(f\"    Std:  {similarities.std():.3f}\")\n",
    "    print(f\"    Min:  {similarities.min():.3f}\")\n",
    "    print(f\"    Max:  {similarities.max():.3f}\")\n",
    "    print()\n",
    "    \n",
    "    # Interpretation\n",
    "    print(\"=\"*70)\n",
    "    print(\"INTERPRETATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if similarities.mean() < 0.5:\n",
    "        print(\"✓ GOOD: Features are diverse (low similarity)\")\n",
    "        print(\"  → Model learned to distinguish different patches\")\n",
    "    elif similarities.mean() < 0.8:\n",
    "        print(\"✓ OK: Features have moderate diversity\")\n",
    "        print(\"  → Model provides useful representations\")\n",
    "    else:\n",
    "        print(\"⚠ WARNING: Features are very similar\")\n",
    "        print(\"  → Model may not have learned well\")\n",
    "    \n",
    "    print()\n",
    "    print(\"Model is ready to use for feature extraction! ✓\")\n",
    "    print(\"=\"*70)\n",
    "    print()\n",
    "    print(\"COMPARISON WITH YOUR SIMCLR:\")\n",
    "    print(f\"  SimCLR features:  512-dim\")\n",
    "    print(f\"  ImageNet features: {feature_dim}-dim\")\n",
    "    print(f\"  ImageNet uses standard normalization and is pretrained on 1.2M images\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67f25403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SIMCLR LINEAR PROBE TEST (WITH REAL LABELS)\n",
      "======================================================================\n",
      "Feature extractor: resnet18\n",
      "✓ SimCLR model loaded\n",
      "\n",
      "Loading labels...\n",
      "✓ Loaded 2123 slide labels\n",
      "  Class distribution: [ 568  486 1069]\n",
      "\n",
      "Loading tiles from 50 slides...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [00:00<00:00, 6346.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 2000 tiles\n",
      "  Class distribution: [520 560 920]\n",
      "\n",
      "Extracting features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 14.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Extracted features: (2000, 512)\n",
      "  Classes: [0 1 2]\n",
      "  Class distribution: [520 560 920]\n",
      "\n",
      "Training linear classifier...\n",
      "\n",
      "======================================================================\n",
      "LINEAR PROBE RESULTS\n",
      "======================================================================\n",
      "Train samples: 1600\n",
      "Test samples:  400\n",
      "\n",
      "Train accuracy: 0.774 (77.4%)\n",
      "Test accuracy:  0.625 (62.5%)\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 65  17  29]\n",
      " [ 25  34  47]\n",
      " [  7  25 151]]\n",
      "\n",
      "Per-class Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.67      0.59      0.62       111\n",
      "     Class 1       0.45      0.32      0.37       106\n",
      "     Class 2       0.67      0.83      0.74       183\n",
      "\n",
      "    accuracy                           0.62       400\n",
      "   macro avg       0.59      0.58      0.58       400\n",
      "weighted avg       0.61      0.62      0.61       400\n",
      "\n",
      "\n",
      "======================================================================\n",
      "INTERPRETATION\n",
      "======================================================================\n",
      "Random baseline (3 classes): 33.3%\n",
      "Your model: 62.5%\n",
      "\n",
      "✓ GOOD - Features are working well\n",
      "  SimCLR learned meaningful representations\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projectnb/ec500kb/projects/Project_1_Team_1/panda_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 200 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=200).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "\n",
    "class TileDataset(Dataset):\n",
    "    def __init__(self, tile_base_dir, label_file, max_slides=50, tiles_per_slide=40, max_samples=2000):\n",
    "        \"\"\"Load tiles with REAL labels from val_set.txt\"\"\"\n",
    "        self.tiles = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Load labels from val_set.txt\n",
    "        print(\"Loading labels...\")\n",
    "        label_dict = {}\n",
    "        with open(label_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) == 2:\n",
    "                        slide_id = parts[0].split('/')[-1]  # Extract ID after 'panda/'\n",
    "                        label = int(parts[1])\n",
    "                        label_dict[slide_id] = label\n",
    "        \n",
    "        print(f\"✓ Loaded {len(label_dict)} slide labels\")\n",
    "        print(f\"  Class distribution: {np.bincount(list(label_dict.values()))}\")\n",
    "        \n",
    "        # Get slide directories\n",
    "        slide_dirs = [d for d in os.listdir(tile_base_dir) \n",
    "                     if os.path.isdir(os.path.join(tile_base_dir, d))]\n",
    "        \n",
    "        # Filter to slides with labels\n",
    "        labeled_slides = [s for s in slide_dirs if s in label_dict][:max_slides]\n",
    "        print(f\"\\nLoading tiles from {len(labeled_slides)} slides...\")\n",
    "        \n",
    "        for slide_id in tqdm(labeled_slides):\n",
    "            label = label_dict[slide_id]\n",
    "            slide_path = os.path.join(tile_base_dir, slide_id)\n",
    "            \n",
    "            # Get tile files\n",
    "            tile_files = [f for f in os.listdir(slide_path) \n",
    "                         if f.endswith('.jpeg') or f.endswith('.jpg')][:tiles_per_slide]\n",
    "            \n",
    "            if len(tile_files) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Add tiles with REAL labels\n",
    "            for tile_file in tile_files:\n",
    "                self.tiles.append(os.path.join(slide_path, tile_file))\n",
    "                self.labels.append(label)\n",
    "            \n",
    "            if len(self.tiles) >= max_samples:\n",
    "                break\n",
    "        \n",
    "        self.tiles = self.tiles[:max_samples]\n",
    "        self.labels = self.labels[:max_samples]\n",
    "        \n",
    "        print(f\"✓ Loaded {len(self.tiles)} tiles\")\n",
    "        print(f\"  Class distribution: {np.bincount(self.labels)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tiles)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.tiles[idx]).convert('RGB')\n",
    "        img = img.resize((224, 224))\n",
    "        img = torch.from_numpy(np.array(img)).permute(2, 0, 1).float() / 255.0\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "        img = (img - mean) / std\n",
    "        return img, self.labels[idx]\n",
    "\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SIMCLR LINEAR PROBE TEST (WITH REAL LABELS)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load SimCLR model\n",
    "from models.resnet_simclr import ResNetSimCLR\n",
    "\n",
    "model = ResNetSimCLR('resnet18', out_dim=512)\n",
    "checkpoint = torch.load('runs/Nov29_15-00-34_scc-214/checkpoints/model.pth', \n",
    "                       map_location='cuda')\n",
    "\n",
    "new_checkpoint = OrderedDict()\n",
    "for k, v in checkpoint.items():\n",
    "    new_checkpoint[k.replace('module.', '')] = v\n",
    "\n",
    "model.load_state_dict(new_checkpoint)\n",
    "model = model.cuda().eval()\n",
    "print(\"✓ SimCLR model loaded\\n\")\n",
    "\n",
    "# Load tiles WITH REAL LABELS\n",
    "tile_dir = '/projectnb/ec500kb/projects/Project_1_Team_1/PANDA_DATA_MANNY/val_tiles'\n",
    "label_file = '/projectnb/ec500kb/projects/Project_1_Team_1/Official_GTP_PANDAS/PANDAS/scripts/val_set.txt'\n",
    "\n",
    "dataset = TileDataset(tile_dir, label_file, max_slides=50, tiles_per_slide=40, max_samples=2000)\n",
    "loader = DataLoader(dataset, batch_size=64, num_workers=4, shuffle=False)\n",
    "\n",
    "# Extract features\n",
    "print(\"\\nExtracting features...\")\n",
    "features_list = []\n",
    "labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, lbls in tqdm(loader):\n",
    "        feats = model.features(imgs.cuda())\n",
    "        feats = torch.flatten(feats, start_dim=1)\n",
    "        features_list.append(feats.cpu().numpy())\n",
    "        labels_list.extend(lbls.numpy())\n",
    "\n",
    "features = np.vstack(features_list)\n",
    "labels = np.array(labels_list)\n",
    "\n",
    "print(f\"\\n✓ Extracted features: {features.shape}\")\n",
    "print(f\"  Classes: {np.unique(labels)}\")\n",
    "print(f\"  Class distribution: {np.bincount(labels)}\")\n",
    "\n",
    "# Train/test split\n",
    "split = int(0.8 * len(features))\n",
    "indices = np.random.permutation(len(features))\n",
    "train_idx = indices[:split]\n",
    "test_idx = indices[split:]\n",
    "\n",
    "X_train, X_test = features[train_idx], features[test_idx]\n",
    "y_train, y_test = labels[train_idx], labels[test_idx]\n",
    "\n",
    "# Linear probe\n",
    "print(\"\\nTraining linear classifier...\")\n",
    "clf = LogisticRegression(max_iter=200, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "train_acc = clf.score(X_train, y_train)\n",
    "test_acc = clf.score(X_test, y_test)\n",
    "\n",
    "# Per-class accuracy\n",
    "y_pred = clf.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LINEAR PROBE RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Train samples: {len(X_train)}\")\n",
    "print(f\"Test samples:  {len(X_test)}\")\n",
    "print(f\"\\nTrain accuracy: {train_acc:.3f} ({train_acc*100:.1f}%)\")\n",
    "print(f\"Test accuracy:  {test_acc:.3f} ({test_acc*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "print(f\"\\nPer-class Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1', 'Class 2']))\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Random baseline (3 classes): 33.3%\")\n",
    "print(f\"Your model: {test_acc*100:.1f}%\")\n",
    "\n",
    "if test_acc < 0.40:\n",
    "    print(\"\\n✗ POOR - Worse than random!\")\n",
    "    print(\"  SimCLR failed completely\")\n",
    "elif test_acc < 0.50:\n",
    "    print(\"\\n⚠ WEAK - Barely better than random\")\n",
    "    print(\"  Features have minimal discriminative power\")\n",
    "elif test_acc < 0.60:\n",
    "    print(\"\\n⚠ MEDIOCRE - Features are weak\")\n",
    "    print(\"  Usable but not great\")\n",
    "elif test_acc < 0.70:\n",
    "    print(\"\\n✓ GOOD - Features are working well\")\n",
    "    print(\"  SimCLR learned meaningful representations\")\n",
    "else:\n",
    "    print(\"\\n✓✓ EXCELLENT - Strong features\")\n",
    "    print(\"  SimCLR is very effective\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4b00777",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cohen_kappa_score, confusion_matrix, classification_report\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GraphDataset\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhelper\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collate\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mGraphTransformer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Classifier\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "# 1. What's in your val_set.txt?\n",
    "!head -20 /projectnb/ec500kb/projects/Project_1_Team_1/Official_GTP_PANDAS/PANDAS/scripts/val_set.txt\n",
    "\n",
    "# 2. Do you have the original PANDA data?\n",
    "!ls /projectnb/ec500kb/projects/Project_1_Team_1/PANDA_DATA_MANNY/\n",
    "\n",
    "# 3. Look for CSV files with labels\n",
    "!find /projectnb/ec500kb/projects/Project_1_Team_1/ -name \"*.csv\" | grep -i label\n",
    "!find /projectnb/ec500kb/projects/Project_1_Team_1/ -name \"*.csv\" | grep -i train\n",
    "\n",
    "# 4. Check PANDA original files\n",
    "!ls /projectnb/ec500kb/projects/Project_1_Team_1/PANDA_DATA_MANNY/ | grep csv\n",
    "\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b0fb1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "panda_env",
   "language": "python",
   "name": "panda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
